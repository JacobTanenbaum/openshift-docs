= Volume Security
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

This topic provides a general guide on pod security context as it relates to
volume security. For information on pod security context in general, please see
link:../../admin_guide/manage_scc.html[Managing Security Context Constraints].

The pod security context (the `*SecurityContext*` stanza in a pod definition)
provides three parameters which allow you to gain and control access to volumes:
`*FSGroup*`,`*SupplementalGroups*`, and `*SELinuxOptions*`.

[IMPORTANT]
====
The `FSGroup` and `SELinuxOptions` parameters are generated for your pod if not
specified. It is recommended that you rely on the automatically generated values
and not specify your own unless you have a specific use case which requires
otherwise.
====

== SupplementalGroups

Supplemental groups are regular Linux groups. When a process runs in Linux, it
has a UID and one or more GIDs. Usually this is just the UID and GIDs of the
user who started the process, but in the case of containers you have the option
of setting these attributes for the container's main process.

The `*SupplementalGroups*` option of the pod `*SecurityContext*` stanza enables
you to specify a list of groups which will be added to the main process of every
container in the pod. This is useful because it enables you to give your pods
group based access to volumes.

For example, consider the following NFS export:

====
----
showmount -e example.com
Export list for example.com:
/nfsexport (everyone)

# ls -ld /nfsexport # on NFS server
drwxrwx---. 2 root 1234 4096 Oct 30 15:27 /nfsexport
----
====

The above export is only accessible by *root* and the group *1234*. So any pod
which is not run as *root* and is not a member the group *1234* will not be able
to access that export. It is often the case that the primary UID in the
container cannot be controlled or predicted.

To solve this issue, you can add the group *1234* to `*SupplementalGroups*` as
in the following example:

====

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: nfs-web
spec:
  containers:
    - name: web
      image: nginx
      ports:
        - name: web
          containerPort: 80
      volumeMounts:
          - name: nfs
            mountPath: "/usr/share/nginx/html"
  securityContext:
    supplementalGroups: [1234]
  volumes:
    - name: nfs
      nfs:
        server: example.com
        path: "/nfsexport"
----
====

All containers in the above pod will be members of the group *1234* and will
have access to the volume regardless of what user the container runs as. This is
useful for volumes which do not allow you to change the export's ownership on
the client, such as NFS and GlusterFS.

== FSGroup

`*FSGroup*` stands for file system group. It is a supplemental group as defined
above, but with some extra functionality. If `*FSGroup*` is specified, it will
have the following effects on volumes which support ownership management:

* The owning group of the volume will be set to the specified `*FSGroup*`.
* Newly created files in the volume will also be owned by `*FSGroup*`.
* Read and write permissions will be given to `*FSGroup*`.
* The specified `*FSGroup*` will be added to the pod's list of supplemental
groups.

This gives your pod, and any other pods with the same `*FSGroup*`, access to the
volume.

[IMPORTANT]
====
Again, it is recommended to allow OpenShift to automatically allocate an
`*FSGroup*` unless you have a specific use case which requires you to override
it in the pod definition.
====

`*FSGroup*` can be specified as follows:

====
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: ebs-web
spec:
  containers:
    - name: web
      image: nginx
      ports:
        - name: web
          containerPort: 80
      volumeMounts:
          - name: ebs-volume
            mountPath: "/usr/share/nginx/html"
  securityContext:
    fsGroup: 1234
  volumes:
    - name: ebs-volume
      awsElasticBlockStore:
      volumeID: <volume_id>
----
====

The above pod will attach the specified AWS EBS volume to the node, format it if
needed, mount it, and connect it to the pod. It will then apply the `*FSGroup*`
to it as detailed above.

Currently the list of volumes which support ownership management includes:

* AWS Elastic Block Store
* OpenStack Cinder
* GCE Persistent Disk
* iSCSI
* emptyDir
* Ceph RBD
* gitRepo

GlusterFS and NFS do not support ownership management.

== SELinuxOptions

The pod security context allows you to specify SELinux labels with which to run
containers in your pod. Additionally, volumes which support SELinux management
will be relabeled so that they are accessible by the specified label and,
depending on how exclusionary the label is, only that label.

This means two things:

* If the container is unprivileged the volume will be given a `*type*` which is
accessible by unprivileged containers. Usually *svirt_sandbox_file_t*.
* If a `*level*` is specified, the volume will be labeled with the given MCS
label.

[NOTE]
====
Level and MCS label are used interchangeably in this topic.
====

For your volume to be accessible by your pod, the pod must have both categories
of the volume. So a pod with *s0:c1,c2* will be able to access volumes with
*s0,c1,c2*, and a volume with *s0* will be accessible by all pods.

[WARNING]
====
Hard coding MCS labels into your pod definition makes it easy for others to
determine what MCS label is needed to access the same volume as the defined pod.
So it is especially important to rely on the MCS labels allocated by OpenShift
and not use this option with care.
====

SELinux options are specified as follows:

====
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: ebs-web
spec:
  containers:
    - name: web
      image: nginx
      ports:
        - name: web
          containerPort: 80
      volumeMounts:
          - name: ebs-volume
            mountPath: "/usr/share/nginx/html"
  securityContext:
    seLinuxOptions:
    level: "s0:c123,c456"
  volumes:
    - name: ebs-volume
      awsElasticBlockStore:
      volumeID: <VOLUME ID>
----
====

Currently the list of volumes which support SELinux management includes:

* AWS Elastic Block Store
* OpenStack Cinder
* GCE Persistent Disk
* iSCSI
* emptyDir
* Ceph RBD
* gitRepo

GlusterFS and NFS do not support SELinux management.

== UID and GID Management with NFS and GlusterFS

As mentioned above, link:persistent_storage_nfs.html[NFS] and
link:persistent_storage_glusterfs.html[GlusterFS] do not support ownership
management. This is because they do not allow `chown` and `chmod` on the client
side. As a result, when you are using NFS and GlusterFS, you must set the
appropriate ownership on the server side, then use `*supplementalGroups*` to
match the group. You can also use `*runAsUser*` to match the user ID.

However, there are a few caveats in this setup that you should be aware of.

=== NFS root_squash Option

NFS usually runs with *root_squash* as a default option. This option tells the
NFS server to squash any attempt to do something using UID 0 to *nfsnobody*. So
if you have a container which is running as *root* and it tries to create a
file, the file will be owned by the *nfsnobody* user.

=== NFS all_squash Option

If the NFS server you are using was set up with the *all_squash* option turned
on, you will not be able to create files which are owned by an arbitrary user or
group. All files will end up being owned by *nfsnobody*.

=== Applications With Strict UID Requirements

Certain applications, such as MySQL, and PostgreSQL, double-check the ownership
of the files they create, and they require that the files be owned by the
application's configured user ID. An application like this cannot be run on an
NFS server which enables *all_squash*, for example, so you would have to turn
that off.

=== NetApp NFSv4 vs NFSv3

NetApp NFSv4 by default enables the *all_squash* option.
https://library.netapp.com/ecmdocs/ECMP1196993/html/GUID-24367A9F-E17B-4725-ADC1-02D86F56F78E.html[This
can be turned off]. However, if you are using NFSv4, NetApp will require that
you setup an authentication system and export `*AUTH_SYSTEM*`. With NFSv3, the
`*AUTH_SYSTEM*` requirement is not strict.
